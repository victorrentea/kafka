spring.application.name: app

logging:
  pattern:
    console: "%d{HH:mm:ss.SSS} [%20.20t] <%-3X{traceId}> %clr(%-5level %-30.30logger{29}) - %msg%n"
  level:
    org.apache.kafka.clients.producer.ProducerConfig: warn
    org.apache.kafka.clients.consumer: warn
    org.apache.kafka.clients.admin.AdminClientConfig: warn
    org.apache.kafka.common.metrics.Metrics: warn
    org.apache.kafka.streams.StreamsConfig: warn
    p6spy: warn # debug #  disable sql log when ran by schedulers
    root: info
server:
  port: 8080
management.endpoints.web.exposure.include: "*"

spring:
  kafka:
    bootstrap-servers: localhost:9092
    producer:
      key-serializer: org.apache.kafka.common.serialization.StringSerializer
      value-serializer: victor.training.kafka.intro.Event$Serializer
      properties:
        interceptor.classes: victor.training.kafka.interceptor.ProducerTraceIdInterceptor
        #acks: 0 # means don't wait for broker ack = fire and forget (default: 1)
    consumer:
      group-id: app
      properties:
        spring.json.trusted.packages: victor.*
        session.timeout.ms: 15000 # default: 45s - no heartbeats from consumer => considered dead. see also heartbeat.interval.ms=3s default
        max.poll.interval.ms: 15000 # =15s (default=5m) = max time between polls. Affects long processing time of a message
        interceptor.classes: victor.training.kafka.interceptor.ConsumerPollAckInterceptor
      key-deserializer: org.apache.kafka.common.serialization.StringDeserializer
      value-deserializer: victor.training.kafka.intro.Event$Deserializer
    listener: #spring-kafka
      concurrency: 2 # number of threads = consumer instances per topic on this machine
#      ack-mode: batch # = after all messages (default) ðŸ˜±risky: on consumer crash during a batch, you are redelivered all messages in that batch
#      ack-mode: record # = after each => less dups on restart vs but throughput loss

#      ack-mode: time # or count, time_count: poll a batch of 500 but ack after every 10 and/or 2 seconds
#      ack-time: 3s
#      ack-count: 3s

#      ack-mode: manual # after batch (or manual_immediate) via Acknowledgement param in @KafkaListener method
      # !!WARNING!! : commit offset 7 => commit all messages before that

  datasource: # external standalone DB instead of embedded one
    url: jdbc:h2:tcp://localhost:9093/~/test
    driver-class-name: org.h2.Driver
    username: sa
    password: sa

  sql:
    init:
      mode: always #to run schema.sql for Schedlock
  jpa:
    defer-datasource-initialization: true #runs schema.sql after hibernate DDL
    generate-ddl: true

decorator:
  datasource:
    p6spy:
      multiline: false
      log-format: "%(executionTime) ms|%(category)|connection %(connectionId)|%(sqlSingleLine)"
      log-filter.pattern: "^(?!.*from inbox i1_0 where i1_0.status='PENDING').*$" # ignore the poller

scheduler-workers:
  core-pool-size: 4
  max-pool-size: ${scheduler-workers.core-pool-size}
  queue-capacity: 0
  thread-name-prefix: schedwork-